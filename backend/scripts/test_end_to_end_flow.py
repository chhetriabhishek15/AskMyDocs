"""
End-to-end test: Upload ‚Üí Chat with Session Persistence

This test verifies the complete flow:
1. Upload a document
2. Wait for processing to complete
3. Chat with the document using a session
4. Verify session persistence across multiple queries
"""
import asyncio
import os
import time
from pathlib import Path
import sys
import uuid

# Add the backend directory to the Python path
sys.path.insert(0, str(Path(__file__).resolve().parent))

import structlog
from dotenv import load_dotenv

# Load environment variables
load_dotenv(Path(__file__).resolve().parent / ".env")

# Setup logging
structlog.configure(
    processors=[
        structlog.stdlib.add_logger_name,
        structlog.stdlib.add_log_level,
        structlog.processors.TimeStamper(fmt="iso"),
        structlog.dev.ConsoleRenderer(),
    ],
    wrapper_class=structlog.stdlib.BoundLogger,
    logger_factory=structlog.stdlib.LoggerFactory(),
)

logger = structlog.get_logger()


async def test_end_to_end_flow():
    """Test complete flow from upload to chat with session persistence."""
    print("=" * 80)
    print("END-TO-END TEST: Upload ‚Üí Chat with Session Persistence")
    print("=" * 80)

    # Generate unique session ID
    session_id = f"test-session-{uuid.uuid4().hex[:8]}"
    print(f"\nüìã Session ID: {session_id}")

    # ============================================================
    # STEP 1: Upload Document
    # ============================================================
    print("\n" + "=" * 80)
    print("STEP 1: Upload Document")
    print("=" * 80)

    try:
        from app.services.upload_service import UploadService
        from app.services.task_tracker import get_task_tracker, TaskStatus

        from app.api.routes.upload import process_document_background
        
        upload_service = UploadService()
        task_tracker = get_task_tracker()  # Use singleton instance

        # Create a test document (markdown file)
        test_doc_content = """# Test Document: RAG System Architecture

## Overview
This document describes the architecture of a Retrieval-Augmented Generation (RAG) system.

## Components

### 1. Document Ingestion
The document ingestion pipeline consists of:
- Document parsing using Docling
- Hybrid chunking with token-aware splitting
- Vector storage in ChromaDB

### 2. Retrieval System
The retrieval system uses:
- Vector similarity search
- ChromaDB for storage
- Configurable similarity thresholds

### 3. LLM Integration
The LLM service:
- Uses LangChain with Google Gemini
- Supports conversation memory via Memori
- Implements response caching

### 4. RAG Pipeline
The complete RAG pipeline:
1. Retrieves relevant chunks
2. Gets conversation history
3. Builds prompts with context
4. Generates responses with sources

## Benefits
- Accurate answers based on document content
- Conversation context preservation
- Source attribution for transparency
"""

        # Save test document
        test_doc_path = Path("test_document.md")
        test_doc_path.write_text(test_doc_content, encoding="utf-8")
        print(f"   ‚úì Created test document: {test_doc_path}")

        # Generate task ID and create task entry (simulating upload endpoint)
        task_id = upload_service.generate_task_id()
        task_tracker.create_task(
            task_id=task_id,
            filename="test_document.md",
            status=TaskStatus.QUEUED,
        )
        print(f"   ‚úì Task created, task_id: {task_id}")

        # Process document (simulating what upload endpoint does)
        # Note: process_document_background is async and will complete before returning
        print("   ‚è≥ Processing document (this may take a moment)...")
        try:
            await process_document_background(
                file_path=test_doc_path,
                filename="test_document.md",
                task_id=task_id,
            )
            
            # Check final status
            task = task_tracker.get_task(task_id)
            if not task:
                print(f"   ‚úó Task {task_id} not found after processing")
                return

            status = task.get("status")
            # Status is already a string, compare directly
            if status == "completed":
                document_id = task.get("document_id")
                print(f"   ‚úì Document processed successfully!")
                print(f"   ‚úì Document ID: {document_id}")
            elif status == "failed":
                error = task.get("error", "Unknown error")
                print(f"   ‚úó Document processing failed: {error}")
                return
            else:
                print(f"   ‚ö†Ô∏è  Unexpected status: {status}")
                print(f"   Task details: {task}")
                # If status is still "queued" or "processing", the background task might not have updated it
                # Let's check if document_id exists (which means processing completed)
                if task.get("document_id"):
                    print(f"   ‚úì Document ID found: {task.get('document_id')} - Processing likely completed")
                else:
                    print(f"   ‚ö†Ô∏è  No document_id found - processing may not have completed")
                    return
                
        except Exception as e:
            print(f"   ‚úó Document processing error: {e}")
            logger.error("document_processing_error", error=str(e), exc_info=True)
            return

        # Clean up test file
        test_doc_path.unlink()
        print(f"   ‚úì Cleaned up test document")

    except Exception as e:
        print(f"   ‚úó Upload step failed: {e}")
        logger.error("upload_step_failed", error=str(e), exc_info=True)
        return

    # ============================================================
    # STEP 2: Initialize RAG Pipeline
    # ============================================================
    print("\n" + "=" * 80)
    print("STEP 2: Initialize RAG Pipeline")
    print("=" * 80)

    try:
        from app.vectorstore.chromadb_store import ChromaDBStore
        from app.repositories.chromadb_repository import ChromaDBRepository
        from app.rag.retrieval import RetrievalService
        from app.services.llm_service import LLMService
        from app.rag.memory_service import MemoryService
        from app.rag.pipeline import RAGPipeline

        chroma_store = ChromaDBStore()
        chroma_repository = ChromaDBRepository(chroma_store)
        retrieval_service = RetrievalService(chroma_repository)
        llm_service = LLMService()
        memory_service = MemoryService()

        pipeline = RAGPipeline(
            retrieval_service=retrieval_service,
            llm_service=llm_service,
            memory_service=memory_service,
        )

        print("   ‚úì RAG Pipeline initialized")
        print(f"   ‚úì Memory service available: {memory_service.is_available()}")

    except Exception as e:
        print(f"   ‚úó Pipeline initialization failed: {e}")
        logger.error("pipeline_init_failed", error=str(e), exc_info=True)
        return

    # ============================================================
    # STEP 3: First Chat Query (No History)
    # ============================================================
    print("\n" + "=" * 80)
    print("STEP 3: First Chat Query (No Conversation History)")
    print("=" * 80)

    try:
        query1 = "What is the RAG system architecture about?"
        print(f"   üìù Query 1: {query1}")

        response1 = await pipeline.generate(
            query=query1,
            session_id=session_id,
            top_k=3,
            min_score=0.0,
        )

        print(f"   ‚úì Response received ({len(response1.answer)} chars)")
        print(f"   ‚úì Sources: {len(response1.sources)} chunks")
        print(f"\n   üí¨ Answer Preview:")
        print(f"   {response1.answer[:300]}...")
        print(f"\n   üìö Sources:")
        for i, source in enumerate(response1.sources[:3], 1):
            print(f"   {i}. {source.get('document_filename', 'unknown')} "
                  f"(similarity: {source.get('similarity_score', 0.0):.2f})")

    except Exception as e:
        print(f"   ‚úó First query failed: {e}")
        logger.error("first_query_failed", error=str(e), exc_info=True)
        return

    # ============================================================
    # STEP 4: Second Chat Query (With History)
    # ============================================================
    print("\n" + "=" * 80)
    print("STEP 4: Second Chat Query (With Conversation History)")
    print("=" * 80)

    try:
        query2 = "What are the main components mentioned?"
        print(f"   üìù Query 2: {query2}")
        print(f"   ‚ÑπÔ∏è  This query should benefit from conversation history")

        response2 = await pipeline.generate(
            query=query2,
            session_id=session_id,  # Same session ID
            top_k=3,
            min_score=0.0,
        )

        print(f"   ‚úì Response received ({len(response2.answer)} chars)")
        print(f"   ‚úì Sources: {len(response2.sources)} chunks")
        print(f"\n   üí¨ Answer Preview:")
        print(f"   {response2.answer[:300]}...")

        # Verify session persistence
        if memory_service.is_available():
            history = memory_service.get_conversation_history(session_id, limit=10)
            print(f"\n   üìú Conversation History: {len(history)} messages")
            if history:
                print("   ‚úì Session memory is working!")
            else:
                print("   ‚ö†Ô∏è  History not retrieved (may be normal if Memori handles it internally)")

    except Exception as e:
        print(f"   ‚úó Second query failed: {e}")
        logger.error("second_query_failed", error=str(e), exc_info=True)
        return

    # ============================================================
    # STEP 5: Third Chat Query (Follow-up)
    # ============================================================
    print("\n" + "=" * 80)
    print("STEP 5: Third Chat Query (Follow-up Question)")
    print("=" * 80)

    try:
        query3 = "Tell me more about the retrieval system"
        print(f"   üìù Query 3: {query3}")
        print(f"   ‚ÑπÔ∏è  This is a follow-up that should use context from previous queries")

        response3 = await pipeline.generate(
            query=query3,
            session_id=session_id,  # Same session ID
            top_k=3,
            min_score=0.0,
        )

        print(f"   ‚úì Response received ({len(response3.answer)} chars)")
        print(f"   ‚úì Sources: {len(response3.sources)} chunks")
        print(f"\n   üí¨ Answer Preview:")
        print(f"   {response3.answer[:300]}...")

    except Exception as e:
        print(f"   ‚úó Third query failed: {e}")
        logger.error("third_query_failed", error=str(e), exc_info=True)
        return

    # ============================================================
    # STEP 6: Verify Prompt Templates Are Used
    # ============================================================
    print("\n" + "=" * 80)
    print("STEP 6: Verify Prompt System Architecture")
    print("=" * 80)

    try:
        from app.rag.prompts.templates import RAGPromptTemplate
        from app.rag.prompts.system_prompts import get_system_prompt

        # Verify prompt templates are accessible
        template = RAGPromptTemplate()
        system_prompt = get_system_prompt("default")

        print("   ‚úì RAGPromptTemplate imported from app.rag.prompts.templates")
        print("   ‚úì System prompts imported from app.rag.prompts.system_prompts")
        print(f"   ‚úì Default system prompt length: {len(system_prompt)} chars")

        # Verify LLM service doesn't have build_rag_prompt
        from app.services.llm_service import LLMService
        llm = LLMService()
        if hasattr(llm, "build_rag_prompt"):
            print("   ‚ö†Ô∏è  WARNING: LLM service still has build_rag_prompt method!")
        else:
            print("   ‚úì LLM service correctly does NOT have build_rag_prompt")

    except Exception as e:
        print(f"   ‚úó Verification failed: {e}")
        logger.error("verification_failed", error=str(e), exc_info=True)

    # ============================================================
    # SUMMARY
    # ============================================================
    print("\n" + "=" * 80)
    print("‚úÖ END-TO-END TEST COMPLETED SUCCESSFULLY!")
    print("=" * 80)
    print(f"\nüìä Summary:")
    print(f"   ‚Ä¢ Session ID: {session_id}")
    print(f"   ‚Ä¢ Document uploaded and processed")
    print(f"   ‚Ä¢ 3 chat queries executed")
    print(f"   ‚Ä¢ Session persistence: {'‚úì Working' if memory_service.is_available() else '‚ö†Ô∏è  Memori not available'}")
    print(f"   ‚Ä¢ Prompt system: ‚úì Using app.rag.prompts templates")
    print(f"   ‚Ä¢ LLM service: ‚úì Clean (no prompt building)")
    print("\n" + "=" * 80)


if __name__ == "__main__":
    asyncio.run(test_end_to_end_flow())

